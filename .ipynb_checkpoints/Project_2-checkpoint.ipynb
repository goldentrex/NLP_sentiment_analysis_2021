{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799900</th>\n",
       "      <td>0</td>\n",
       "      <td>2329170385</td>\n",
       "      <td>Thu Jun 25 10:26:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>celhouston855</td>\n",
       "      <td>has a feeling the rest of the day is going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799901</th>\n",
       "      <td>0</td>\n",
       "      <td>2329170393</td>\n",
       "      <td>Thu Jun 25 10:26:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>maryag</td>\n",
       "      <td>@JapanNewbie woohoo I'm all for slack time.. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799902</th>\n",
       "      <td>0</td>\n",
       "      <td>2329170631</td>\n",
       "      <td>Thu Jun 25 10:26:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ianbicknell</td>\n",
       "      <td>Scratch that.. No pool today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799903</th>\n",
       "      <td>0</td>\n",
       "      <td>2329170797</td>\n",
       "      <td>Thu Jun 25 10:26:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>44ava_182</td>\n",
       "      <td>@markhoppus  my girlfriend just dumped me, got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799904</th>\n",
       "      <td>0</td>\n",
       "      <td>2329171365</td>\n",
       "      <td>Thu Jun 25 10:26:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MasterAbbott</td>\n",
       "      <td>It's 3:30 am and I can't sleep  I'm sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label        time                          date     query  \\\n",
       "799900      0  2329170385  Thu Jun 25 10:26:00 PDT 2009  NO_QUERY   \n",
       "799901      0  2329170393  Thu Jun 25 10:26:00 PDT 2009  NO_QUERY   \n",
       "799902      0  2329170631  Thu Jun 25 10:26:02 PDT 2009  NO_QUERY   \n",
       "799903      0  2329170797  Thu Jun 25 10:26:02 PDT 2009  NO_QUERY   \n",
       "799904      0  2329171365  Thu Jun 25 10:26:04 PDT 2009  NO_QUERY   \n",
       "\n",
       "             username                                               text  \n",
       "799900  celhouston855  has a feeling the rest of the day is going to ...  \n",
       "799901         maryag  @JapanNewbie woohoo I'm all for slack time.. I...  \n",
       "799902    ianbicknell                      Scratch that.. No pool today   \n",
       "799903      44ava_182  @markhoppus  my girlfriend just dumped me, got...  \n",
       "799904   MasterAbbott           It's 3:30 am and I can't sleep  I'm sad   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[799900:800100]\n",
    "N=len(data)\n",
    "print('Number of tweets: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the text and label coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['text','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning 1 to Positive sentment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'][data['label']==4]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking one fourth data so we can run on our machine easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making statement text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick  spending my day laying in bed listening ...\n",
       "799995                                      gmail is down? \n",
       "799996                        rest in peace farrah! so sad \n",
       "799997    @eric_urbane sounds like a rival is flagging y...\n",
       "799998    has to resit exams over summer...  wishes he w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and removing Stop words of english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and removing the above stop words list from the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                love @health4uandpets u guys r best!!\n",
       "800000    im meeting one besties tonight! cant wait!! - ...\n",
       "800001    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                      @lovesbrooklyn2 effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick spending day laying bed listening taylors...\n",
       "799995                                           gmail down\n",
       "799996                                rest peace farrah sad\n",
       "799997    ericurbane sounds like rival flagging ads much...\n",
       "799998    resit exams summer wishes worked harder first ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick spending day laying bed listening taylors...\n",
       "799995                                           gmail down\n",
       "799996                                 rest peace farah sad\n",
       "799997    ericurbane sounds like rival flaging ads much ...\n",
       "799998    resit exams sumer wishes worked harder first y...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and removing email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_email(data):\n",
    "    return re.sub('@[^\\s]+', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick spending day laying bed listening taylors...\n",
       "799995                                           gmail down\n",
       "799996                                 rest peace farah sad\n",
       "799997    ericurbane sounds like rival flaging ads much ...\n",
       "799998    resit exams sumer wishes worked harder first y...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and removing URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick spending day laying bed listening taylors...\n",
       "799995                                           gmail down\n",
       "799996                                 rest peace farah sad\n",
       "799997    ericurbane sounds like rival flaging ads much ...\n",
       "799998    resit exams sumer wishes worked harder first y...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and removing Numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799994    sick spending day laying bed listening taylors...\n",
       "799995                                           gmail down\n",
       "799996                                 rest peace farah sad\n",
       "799997    ericurbane sounds like rival flaging ads much ...\n",
       "799998    resit exams sumer wishes worked harder first y...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tokenization of tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999             [love, healthuandpets, u, guys, r, best]\n",
       "800000    [im, meting, one, besties, tonight, cant, wait...\n",
       "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800002    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800003                      [lovesbroklyn, efect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: stemming_on_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999             [love, healthuandpets, u, guys, r, best]\n",
       "800000    [im, meting, one, besties, tonight, cant, wait...\n",
       "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800002    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800003                      [lovesbroklyn, efect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999             [love, healthuandpets, u, guys, r, best]\n",
       "800000    [im, meting, one, besties, tonight, cant, wait...\n",
       "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800002    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800003                      [lovesbroklyn, efect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating input feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.text\n",
    "y=data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the input features for training\n",
    "We converting the text words into arrays form.\n",
    "Maximum 500 features/words selected for training. These 500 words will be selected on the importance that will distinguish between the positive tweets and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 500)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "    layer = LSTM(64)(layer) #step3\n",
    "    layer = Dense(256,name='FC1')(layer) #step4\n",
    "    layer = Activation('relu')(layer) # step5\n",
    "    layer = Dropout(0.5)(layer) # step6\n",
    "    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
    "    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
    "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
    "    return model #function returning the value when we call it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are calling the model\n",
    "We are using 2 classes so we set \"binary_crossentropy\" and if we use more than two classes then we use \"categorical_crossentropy\"\n",
    "Optimizer is a function that used to change the features of neural network such as learning rate (how the model learn with features) in order to reduce the losses. So the learning rate of neural network to reduce the losses is defined by optimizer.\n",
    "We are setting metrics=accuracy because we are going to caluclate the percentage of correct predictions over all predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow_based_model() # here we are calling the function of created model\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validating with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2/2 [==============================] - 3s 965ms/step - loss: 0.6928 - accuracy: 0.5337 - val_loss: 0.6995 - val_accuracy: 0.3571\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.6877 - accuracy: 0.5284 - val_loss: 0.6986 - val_accuracy: 0.3571\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 0.6785 - accuracy: 0.5284 - val_loss: 0.6911 - val_accuracy: 0.4286\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.6610 - accuracy: 0.8570 - val_loss: 0.7444 - val_accuracy: 0.3571\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 0.6358 - accuracy: 0.6247 - val_loss: 0.7016 - val_accuracy: 0.4286\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 0.5842 - accuracy: 0.8865 - val_loss: 0.7907 - val_accuracy: 0.3571\n",
      "Training finished !!\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
    "print('Training finished !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7078 - accuracy: 0.4667\n"
     ]
    }
   ],
   "source": [
    "accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting prediction of the test data and then we will compare the true labels/classes of the data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) #getting predictions on the trained model\n",
    "y_pred = (y_pred > 0.5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
