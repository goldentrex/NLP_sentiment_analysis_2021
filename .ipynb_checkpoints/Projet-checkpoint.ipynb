{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using RNN and word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary :\n",
    "\n",
    "- [Importing data](#1)\n",
    "- [Preprocessing](#2)\n",
    "- [Tokenizer](#3)\n",
    "- [Split of train and test data](#4)\n",
    "- [Compile the model](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database at https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written in Python3, the goal was to analyse a 16.000.000 tweet dataset using RNN and word embedding to do sentiment analysis\n",
    "\n",
    "The notebook is organised like this : titles are in bold typo and shortcuts are available in the summary, explainations about what we are doing are in comments (markdowns) and notes of few things I tried and worked with but didn't actually work in the end are in brut text (for NBconvert) and the code inside it wont compute\n",
    "\n",
    "And here we go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/victorgaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing file data.csv with all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0           1                             2         3  \\\n",
      "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...     ..         ...                           ...       ...   \n",
      "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                       4                                                  5  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/data.csv\", sep=',', encoding = 'latin', header=None)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to take only 1000 positive and 1000 negative tweets to get faster calculation just taking from the middle of the dataset but all the time I was trying to compile the model it didn't work, I finaly used the iloc function from the pandas library (few lines later) to be able to use less tweets and for my model to run properly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "data = data[799000:801000]\n",
    "N=len(data)\n",
    "print('Number of tweets: ', N)\n",
    "data.head()\n",
    "\"\"\"\n",
    "\n",
    "# code to try the division of length, unused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename the columns for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "           user_id                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't need all the columns, let's drop the ones we don't want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          0\n",
       "1  is upset that he can't update his Facebook by ...          0\n",
       "2  @Kenichan I dived many times for the ball. Man...          0\n",
       "3    my whole body feels itchy and like its on fire           0\n",
       "4  @nationwideclass no, it's not behaving at all....          0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['text','sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data[data.sentiment == 4]\n",
    "neg = data[data.sentiment == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take only 5000 positive and 5000 negative tweets to get faster calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos.iloc[:int(5000)]\n",
    "neg = neg.iloc[:int(5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign 1 to positive instead of 4 so it makes our testing possible later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.sentiment = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>long day today</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>a friend broke his promises..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>@gjarnling I am fine thanks - tired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>trying to keep my eyes open..damn baking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>why the hell is it snowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  sentiment\n",
       "800000       I LOVE @Health4UandPets u guys r the best!!           1\n",
       "800001  im meeting up with one of my besties tonight! ...          1\n",
       "800002  @DaRealSunisaKim Thanks for the Twitter add, S...          1\n",
       "800003  Being sick can be really cheap when it hurts t...          1\n",
       "800004    @LovesBrooklyn2 he has that effect on everyone           1\n",
       "...                                                   ...        ...\n",
       "4995                                      long day today           0\n",
       "4996                       a friend broke his promises..           0\n",
       "4997                 @gjarnling I am fine thanks - tired           0\n",
       "4998            trying to keep my eyes open..damn baking           0\n",
       "4999                          why the hell is it snowing           0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([pos, neg])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the .csv, the tweets with negative meaning is labeled as a 0, and positive as a 4, so we will replace 0 with word negative, 4 with the word positive for better understanding of the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\"\n",
    "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
    "def label_decoder(label):\n",
    "    return lab_to_sentiment[label]\n",
    "\n",
    "data.sentiment = data.sentiment.apply(lambda x: label_decoder(x))\n",
    "\"\"\"\n",
    "\n",
    "# unused in the end for the accuracy test I needed to have integers and no string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start preprocessing, we want to make our tweets in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000         i love @health4uandpets u guys r the best!! \n",
       "800001    im meeting up with one of my besties tonight! ...\n",
       "800002    @darealsunisakim thanks for the twitter add, s...\n",
       "800003    being sick can be really cheap when it hurts t...\n",
       "800004      @lovesbrooklyn2 he has that effect on everyone \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text=data.text.str.lower()\n",
    "\n",
    "data.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess our data, we will want to lemmatize and to remove stop words to keep only the most important of our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in nltk.corpus.stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                love @health4uandpets u guys r best!!\n",
       "800001    im meeting one besties tonight! cant wait!! - ...\n",
       "800002    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800003    sick really cheap hurts much eat real food plu...\n",
       "800004                      @lovesbrooklyn2 effect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                friend broke promises..\n",
       "4997                         @gjarnling fine thanks - tired\n",
       "4998                     trying keep eyes open..damn baking\n",
       "4999                                           hell snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to remove the characters which are repeated in the words so it is even cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_repeating_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                 love @health4uandpets u guys r best!\n",
       "800001    im meting one besties tonight! cant wait! - gi...\n",
       "800002    @darealsunisakim thanks twiter ad, sunisa! got...\n",
       "800003    sick realy cheap hurts much eat real fod plus,...\n",
       "800004                        @lovesbroklyn2 efect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                 friend broke promises.\n",
       "4997                         @gjarnling fine thanks - tired\n",
       "4998                       trying kep eyes open.damn baking\n",
       "4999                                            hel snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to clean and remove names or emails, which are always after the symbol \"@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_at(data):\n",
    "    return re.sub('@[^\\s]+', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_at(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                                love   u guys r best!\n",
       "800001    im meting one besties tonight! cant wait! - gi...\n",
       "800002      thanks twiter ad, sunisa! got met hin show d...\n",
       "800003    sick realy cheap hurts much eat real fod plus,...\n",
       "800004                                       efect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                 friend broke promises.\n",
       "4997                                    fine thanks - tired\n",
       "4998                       trying kep eyes open.damn baking\n",
       "4999                                            hel snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean and remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLs(data):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_URLs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                                love   u guys r best!\n",
       "800001    im meting one besties tonight! cant wait! - gi...\n",
       "800002      thanks twiter ad, sunisa! got met hin show d...\n",
       "800003    sick realy cheap hurts much eat real fod plus,...\n",
       "800004                                       efect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                 friend broke promises.\n",
       "4997                                    fine thanks - tired\n",
       "4998                       trying kep eyes open.damn baking\n",
       "4999                                            hel snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                                love   u guys r best!\n",
       "800001    im meting one besties tonight! cant wait! - gi...\n",
       "800002      thanks twiter ad, sunisa! got met hin show d...\n",
       "800003    sick realy cheap hurts much eat real fod plus,...\n",
       "800004                                       efect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                 friend broke promises.\n",
       "4997                                    fine thanks - tired\n",
       "4998                       trying kep eyes open.damn baking\n",
       "4999                                            hel snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                                 love   u guys r best\n",
       "800001    im meting one besties tonight cant wait  girl ...\n",
       "800002      thanks twiter ad sunisa got met hin show dc ...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                                       efect everyone\n",
       "                                ...                        \n",
       "4995                                         long day today\n",
       "4996                                  friend broke promises\n",
       "4997                                     fine thanks  tired\n",
       "4998                        trying kep eyes opendamn baking\n",
       "4999                                            hel snowing\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of tweets text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data.text = data.text.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                             [love, u, guys, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [thanks, twiter, ad, sunisa, got, met, hin, sh...\n",
       "800003    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800004                                    [efect, everyone]\n",
       "                                ...                        \n",
       "4995                                     [long, day, today]\n",
       "4996                              [friend, broke, promises]\n",
       "4997                                  [fine, thanks, tired]\n",
       "4998                  [trying, kep, eyes, opendamn, baking]\n",
       "4999                                         [hel, snowing]\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(data):\n",
    "    text = [nltk.PorterStemmer().stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: stemming_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                             [love, u, guys, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [thanks, twiter, ad, sunisa, got, met, hin, sh...\n",
       "800003    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800004                                    [efect, everyone]\n",
       "                                ...                        \n",
       "4995                                     [long, day, today]\n",
       "4996                              [friend, broke, promises]\n",
       "4997                                  [fine, thanks, tired]\n",
       "4998                  [trying, kep, eyes, opendamn, baking]\n",
       "4999                                         [hel, snowing]\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finaly we apply the lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_data(data):\n",
    "    text = [nltk.WordNetLemmatizer().lemmatize(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: lemmatizing_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                             [love, u, guys, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [thanks, twiter, ad, sunisa, got, met, hin, sh...\n",
       "800003    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800004                                    [efect, everyone]\n",
       "                                ...                        \n",
       "4995                                     [long, day, today]\n",
       "4996                              [friend, broke, promises]\n",
       "4997                                  [fine, thanks, tired]\n",
       "4998                  [trying, kep, eyes, opendamn, baking]\n",
       "4999                                         [hel, snowing]\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Split of train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to shuffle the dataset for it to be randomized, train_test_split will shuffle the dataset for us and split it to gives training and testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we separate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.text\n",
    "y=data.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now prepare the input features for training \n",
    "\n",
    "We convert the text words into a matrix with a maximum of 300 features per word selected for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(x)\n",
    "sequences = tok.texts_to_sequences(x)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use train_test_split to have our data splited for the training and testing, x being the text and y the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our model, which will be made of an input layer, an embedding layer, then LSTM, a dense layer, an activation using ReLU, then the dropout and an other dense layer before finishing with a last layer of activation using sigmoid this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "    layer = LSTM(64)(layer) #step3\n",
    "    layer = Dense(256,name='FC1')(layer) #step4\n",
    "    layer = Activation('relu')(layer) # step5\n",
    "    layer = Dropout(0.5)(layer) # step6\n",
    "    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
    "    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
    "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
    "    return model #function returning the value when we call it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to call our model, so will be using 2 classes, if we set \"binary_crossentropy\" and use more than two classes then we will be using \"categorical_crossentropy\" \n",
    "\n",
    "We can change the features of neural network such as learning rate with the optimizer function in order to reduce the losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tensorflow_based_model() # here we are calling the function of created model\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train and validate the model with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "79/79 [==============================] - 25s 287ms/step - loss: 0.6821 - accuracy: 0.5564 - val_loss: 0.5967 - val_accuracy: 0.6671\n",
      "Epoch 2/6\n",
      "79/79 [==============================] - 22s 282ms/step - loss: 0.5296 - accuracy: 0.7474 - val_loss: 0.6046 - val_accuracy: 0.6843\n",
      "Epoch 3/6\n",
      "79/79 [==============================] - 24s 307ms/step - loss: 0.4603 - accuracy: 0.7848 - val_loss: 0.6193 - val_accuracy: 0.6857\n",
      "Epoch 4/6\n",
      "79/79 [==============================] - 28s 351ms/step - loss: 0.4240 - accuracy: 0.8063 - val_loss: 0.6192 - val_accuracy: 0.6786\n",
      "Epoch 5/6\n",
      "79/79 [==============================] - 26s 324ms/step - loss: 0.4062 - accuracy: 0.8187 - val_loss: 0.6325 - val_accuracy: 0.6771\n",
      "Epoch 6/6\n",
      "79/79 [==============================] - 23s 295ms/step - loss: 0.3846 - accuracy: 0.8300 - val_loss: 0.6579 - val_accuracy: 0.6800\n",
      "Training finished !!\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
    "print('Training finished !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 4s 41ms/step - loss: 0.6284 - accuracy: 0.7080\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x_test,y_test) #we are starting to test the model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Accuracy: {:0.2f}'.format(accuracy[1])) #the accuracy of the model on test data is given below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
